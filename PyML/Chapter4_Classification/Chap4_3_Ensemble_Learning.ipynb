{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chap4_3_Ensemble Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPXjJARnSc9+EaHCbiTElcj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PyBlin/Study/blob/main/PyML/Chapter4_Classification/Chap4_3_Ensemble_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvpqbpkD0RBY"
      },
      "source": [
        "# 3.Ensemble Learning\n",
        "\n",
        "* Voting\n",
        ">* 서로 다른 알고리즘을 가진 분류기를 결합합니다.\n",
        ">* ex) 하나의 데이터셋을 각각 LR, k-NN, SVM을 사용하여 Voting 합니다.\n",
        "\n",
        "* Bagging\n",
        ">* 각각의 분류기가 모두 같은 유형의 알고리즘 기반이지만, 데이터 샘플링이 다릅니다.\n",
        ">* 대표적으로 Random Forest 알고리즘이 있습니다.\n",
        ">* 이렇게 개별 분류기에게 원본 데이터를 샘플링해서 추출하는 방식을 \"Bootstrapping 분할 방식\"이라고 합니다.\n",
        ">* \"Bootstrapping\" 방식으로 샘플링된 데이터셋에 대해서 학습하고 예측한 결과를 \"Voting\"을 통해서 최종 예측 결과를 선정하는 방식을 \"Bagging Ensemble\"입니다.\n",
        ">* 교차 검증과 다르게, Bagging은 데이터셋간에 중첩을 허용합니다.\n",
        ">* 즉, 1만개의 데이터를 10개의 분류기가 Bagging 하더라도, 각 1천개의 데이터 내에는 중복된 데이터가 있습니다.\n",
        ">* ex) 여러 개의 데이터셋을 모두 DT를 사용하여 Bagging 합니다.\n",
        "\n",
        "* Boosting\n",
        ">* 여러 개의 같은 분류기가 순차적으로 학습을 수행하되, 앞에서 학습한 분류기가 예측이 틀린 데이터에 대해서는 올바르게 예측할 수 있도록 다음 분류기에게 가중치( weight )를 부여합니다.\n",
        ">* 계속해서 가중치를 부여해서 Boosting이라고 합니다.\n",
        ">* 예측 성능이 뛰어나 Ensemble 학습을 주도하고 있습니다.\n",
        ">* 대표적으로 Gradient Boost, XGBoost, LightGBM 등이 있습니다.\n",
        "\n",
        "* Stacking\n",
        ">* 여러 모델의 예측 결과값을 다시 학습 데이터로 만들어서 다른 모델( meta model)로 재학습시키는 방식입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHNd-bxYUEdT"
      },
      "source": [
        "## 3.1 Hard Voting & Soft Voting\n",
        "\n",
        "* Hard Voting\n",
        ">* 다수결과 비슷합니다.\n",
        ">* 예측한 결과값 중 다수의 분류기가 결정한 예측값을 최종으로 선정합니다.\n",
        "\n",
        "* Soft Voting\n",
        ">* 분류기들의 레이블 결정값을 모두 더하고 평균 중에서 확률이 가장 높은 레이블 값을 최종으로 선정합니다.\n",
        ">* 일반적으로 많이 적용됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvuj4gA4UyM8"
      },
      "source": [
        "## 3.2 Voting Classifier\n",
        "\n",
        "* 사이킷런은 Voting Ensemble을 구현한 `VotingClassifier` 클래스를 제공합니다.\n",
        "* 위스콘신 유방암 데이터셋을 예측해 봅시다!\n",
        "* 악성/양성 종양 여부를 결정하는 이진 분류 데이터셋이며, 종양의 크기, 모양 등의 형태와 관련한 많은 피처가 있습니다.\n",
        "* Logistic Regression, KNN 기반으로 수행해 봅시다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "0IqW32uPVObL",
        "outputId": "d99042ab-90b9-4b6c-f933-3702b5105c92"
      },
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "data_df.head()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>radius error</th>\n",
              "      <th>texture error</th>\n",
              "      <th>perimeter error</th>\n",
              "      <th>area error</th>\n",
              "      <th>smoothness error</th>\n",
              "      <th>compactness error</th>\n",
              "      <th>concavity error</th>\n",
              "      <th>concave points error</th>\n",
              "      <th>symmetry error</th>\n",
              "      <th>fractal dimension error</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n",
              "0        17.99         10.38  ...          0.4601                  0.11890\n",
              "1        20.57         17.77  ...          0.2750                  0.08902\n",
              "2        19.69         21.25  ...          0.3613                  0.08758\n",
              "3        11.42         20.38  ...          0.6638                  0.17300\n",
              "4        20.29         14.34  ...          0.2364                  0.07678\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoCWuxreWd60",
        "outputId": "b6e588b4-7f7f-4122-b4d6-0d39e644e920"
      },
      "source": [
        "# 개별 모델은 Logistic Regressin, KNN\n",
        "lr_clf = LogisticRegression()\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=8)\n",
        "\n",
        "# 개별 모델을 soft voting 기반의 ensemble 모델로 구현한 분류기\n",
        "vo_clf = VotingClassifier(estimators=[('LR', lr_clf), ('KNN', knn_clf)], \n",
        "                          voting='soft')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=156)\n",
        "\n",
        "# Voting Classifier 학습/예측/평가\n",
        "vo_clf.fit(X_train, y_train)\n",
        "pred = vo_clf.predict(X_test)\n",
        "print(f\"Voting Classifier Accuracy : {accuracy_score(y_test, pred):.4f}\")\n",
        "\n",
        "# 개별 모델의 학습/예측/평가\n",
        "classifiers = [lr_clf, knn_clf]\n",
        "for classifier in classifiers:\n",
        "    classifier.fit(X_train, y_train)\n",
        "    pred = classifier.predict(X_test)\n",
        "    class_name = classifier.__class__.__name__\n",
        "    print(f\"{class_name} Accuracy : {accuracy_score(y_test, pred):.4f}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Classifier Accuracy : 0.9474\n",
            "LogisticRegression Accuracy : 0.9386\n",
            "KNeighborsClassifier Accuracy : 0.9386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_czz5FeWYfFL"
      },
      "source": [
        "* Voting 분류기가 조금 높은데, 무조건 향상되는건 아닙니다. 오히려 기반 분류기가 가장 성능이 좋을 수도 있습니다.\n",
        "* 그럼에도 불구하고 단일 ML 알고리즘보다 뛰어납니다.\n",
        "* 고정된 데이터셋에서 단일 ML 알고리즘이 뛰어날지라도, 현실에서는 어렵습니다.\n",
        "* 현실에서도 똑똑한 사람들로만 구성된 팀보다는 다양한 경험의 백그라운드를 가진 사람들로 구성된 팀이 더 성공할 가능성이 높습니다.\n",
        "* 높은 유연성은 해결책을 제시할 수 있습니다.\n",
        "* 이런 관점에서 편향-분산 Trade-off는 ML 모델이 극복해야 할 중요 과제입니다."
      ]
    }
  ]
}